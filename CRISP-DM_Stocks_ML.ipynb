{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "#Recommended Python version: 3.11.9\n",
    "\n",
    "!python --version\n",
    "!python -c \"import platform; print(platform.architecture())\"\n",
    "!pip install --upgrade pip\n",
    "!git --version\n",
    "\n",
    "# !pip install -q yfinance\n",
    "# !pip install -q pandas\n",
    "# !pip install -q matplotlib\n",
    "# !pip install -q seaborn\n",
    "# !pip install -q pandas_datareader\n",
    "# !pip install -q setuptools\n",
    "# !pip install -q scikit-learn\n",
    "# !pip install -q keras\n",
    "# !pip install -q jupyter pandas pmaw\n",
    "# !pip install vaderSentiment\n",
    "#!pip install -q plotly\n",
    "\n",
    "# !pip install tensorflow\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd back\n",
    "!pip install -r requirements.txt\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Application of the CRISP-DM Model to Analyze Stocks Using ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Business Understanding**\n",
    "#### The main goal of the stock analysis tool is to support investors (e.g., retail investors) in making informed investment decisions by providing accurate, data-driven analyses and predictions about stock prices, market trends, and potential investment opportunities.\n",
    "\n",
    "**Business Objectives:**\n",
    "- Increase returns: Assist users in identifying profitable investment opportunities.\n",
    "- Risk minimization: Provide tools to assess risks (e.g., volatility, market risks).\n",
    "- Time savings: Automate analyses that are typically performed manually by analysts.\n",
    "- Competitive advantage: Develop a tool that stands out from existing solutions through accuracy, user-friendliness, and innovative features.\n",
    "\n",
    "**Background:**\n",
    "The stock market is complex and influenced by a variety of factors, including:\n",
    "- Economic indicators,\n",
    "- Company metrics,\n",
    "- News,\n",
    "- Geopolitical events,\n",
    "- Market sentiment.\n",
    "Many investors rely on traditional analysis methods (e.g., fundamental and technical analysis), which can be time-consuming and subjective. Data-driven tools leveraging machine learning and big data offer the opportunity to automate these processes and deliver more precise results.\n",
    "\n",
    "**Current Challenges:**\n",
    "- Data quality: Availability and reliability of financial data (e.g., stock prices, company reports).\n",
    "- Market dynamics: Rapid market changes require real-time or near-real-time analyses.\n",
    "- Complexity: Difficulty in integrating various data sources (e.g., social media, news, financial reports).\n",
    "- Competition: Existing tools like Bloomberg Terminal, TradingView, or Robinhood set standards that must be surpassed.\n",
    "\n",
    "**Opportunities:**\n",
    "- Use of modern technologies such as machine learning, natural language processing (NLP), and sentiment analysis.\n",
    "- Integration of alternative data sources (e.g., X posts, news, macroeconomic data).\n",
    "- Customization for specific target groups (e.g., day traders, long-term investors, or institutional investors).\n",
    "\n",
    "**Project Objectives:**\n",
    "- Predictive model: Develop a model to forecast stock prices or market trends (e.g., price movements in the next days/weeks/months/years).\n",
    "- Risk analysis: Provide risk assessments for individual stocks or portfolios.\n",
    "- Market sentiment: Analyze sentiment based on news and social media (e.g., X posts).\n",
    "- Automation: Automatically generate reports or actionable recommendations (buy, sell, hold).\n",
    "\n",
    "**Success Criteria from a Data-Driven Perspective:**\n",
    "- Prediction accuracy: 75% hit rate for price forecasts.\n",
    "- Performance: Process large datasets in near real-time (e.g., analysis of stock prices and news within seconds).\n",
    "- User-friendliness: Provide clear visualizations and understandable recommendations.\n",
    "\n",
    "**Requirements:**\n",
    "- Data sources: \n",
    "    - Historical stock prices,\n",
    "    - Financial reports,\n",
    "    - Macroeconomic data,\n",
    "    - News,\n",
    "    - Social media (e.g., X posts).\n",
    "- Technology: \n",
    "    - Cloud computing for scalability,\n",
    "    - Machine learning for predictions,\n",
    "    - NLP for sentiment analysis.\n",
    "\n",
    "**Assumptions:**\n",
    "- High-quality data is available and can be obtained in sufficient quantities.\n",
    "- Historical data is representative of future market conditions (with limitations due to unpredictable events such as crises)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Understanding**\n",
    "#### \n",
    "**Required Data Sources:**\n",
    "To achieve the business objectives (e.g., stock price predictions, risk analyses, market sentiment assessments), various data sources are needed. These can be divided into the following categories:\n",
    "\n",
    "- Market data:\n",
    "    - Historical stock prices (e.g., open, close, high, low, volume).\n",
    "    - Indices (e.g., DAX, S&P 500, NASDAQ).\n",
    "    - Commodity prices (e.g., oil, gold) that may influence stocks.\n",
    "    - Exchange rates (e.g., EUR/USD, USD/JPY).\n",
    "- Company data:\n",
    "    - Financial reports (e.g., income statements, balance sheets, cash flow).\n",
    "    - Metrics (e.g., P/E ratio, dividend yield, equity ratio).\n",
    "    - Insider trading and stock buybacks.\n",
    "- Macroeconomic data:\n",
    "    - Interest rates (e.g., from central banks like ECB, Fed).\n",
    "    - Inflation rates.\n",
    "    - Labor market data (e.g., unemployment rate).\n",
    "    - GDP growth.\n",
    "- Alternative data:\n",
    "    - News (e.g., press releases, market reports).\n",
    "    - Social media data (e.g., X posts reflecting market sentiment).\n",
    "    - Sentiment data (e.g., Fear and Greed Index).\n",
    "- Technical data:\n",
    "    - Technical indicators (e.g., moving averages, RSI, MACD).\n",
    "    - Trading volume and liquidity metrics.\n",
    "\n",
    "**Data Sources:**\n",
    "- Financial data APIs: Yahoo Finance, Alpha Vantage, Quandl, Bloomberg API.\n",
    "- News APIs: Reuters, NewsAPI.\n",
    "- Social media: X API (for posts, hashtags, and trends).\n",
    "- Macroeconomic data: OECD, World Bank, FRED (Federal Reserve Economic Data).\n",
    "- Company data: SEC EDGAR database (for U.S. companies), company websites.\n",
    "\n",
    "**Data Attributes:**\n",
    "- Market data: Date, stock price (Open, Close, High, Low), volume.\n",
    "- Company data: Revenue, profit, debt, equity, industry.\n",
    "- Macroeconomic data: Interest rate, inflation, GDP.\n",
    "- News/Social media: Text, date.\n",
    "\n",
    "**Data Availability:**\n",
    "- Market data: Widely available through APIs like Yahoo Finance or Alpha Vantage, sometimes paid for real-time data.\n",
    "- Company data: Available via SEC EDGAR or financial data APIs, but with limitations for smaller companies or non-U.S. firms.\n",
    "- News/Social media: Available through APIs (e.g., X API), but associated with costs and rate limits.\n",
    "- Macroeconomic data: Free through public sources (e.g., FRED), but often delayed.\n",
    "\n",
    "**Feasibility:**\n",
    "- Technical feasibility: Data can be processed with Python libraries (e.g., Pandas, NumPy, Scikit-learn) and cloud computing (e.g., AWS).\n",
    "- Data integration: The challenge lies in integrating structured (e.g., stock prices) and unstructured data (e.g., news).\n",
    "- Modeling: Machine learning (e.g., time series models like ARIMA, LSTM) and NLP are suitable for achieving the business objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import os\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import clear_output\n",
    "from scipy.signal import argrelextrema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **get_Ticker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_ticker(file_existing_ticker, symbol, file_new_ticker, symbol_name):\n",
    "    ticker = pd.read_csv(file_existing_ticker)\n",
    "    lst_ticker = ticker[symbol].tolist()\n",
    "    print(len(lst_ticker))\n",
    "\n",
    "    new_tickers = pd.read_csv(file_new_ticker, encoding_errors=\"replace\")\n",
    "    lst = new_tickers[symbol_name].tolist()\n",
    "\n",
    "    lst_ticker = lst_ticker + lst\n",
    "    lst_ticker = pd.unique(lst_ticker).tolist()\n",
    "    print(len(lst_ticker))\n",
    "\n",
    "    df_symbols = pd.DataFrame({\"ticker\": lst_ticker})\n",
    "\n",
    "    if os.path.exists(f'ticker_new.csv'):\n",
    "        os.remove(f'ticker_new.csv')\n",
    "\n",
    "    df_symbols.to_csv(f'ticker_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate if necessary\n",
    "# file_existing_ticker = 'ticker_new.csv'\n",
    "# symbol = 'ticker'\n",
    "\n",
    "# file_new_ticker = 'Taiwan.csv'\n",
    "# symbol_name = 'Ticker'\n",
    "\n",
    "# get_new_ticker(file_existing_ticker, symbol, file_new_ticker, symbol_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **yfinance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbols(var, file_data):\n",
    "    try:\n",
    "        df = pd.read_csv(f'{var}_new.csv')\n",
    "        if var not in df.columns:\n",
    "            raise ValueError(f'CSV file must contain a {var} column')\n",
    "        lst = df[var].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {var}.csv: {str(e)}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    if os.path.exists(file_data):\n",
    "        existing = pd.read_csv(file_data)\n",
    "        # Group existing_data by var\n",
    "        dfs_dict = {symbol: group.sort_values('Date') for symbol, group in existing.groupby('Symbol')}\n",
    "    else:\n",
    "        existing = pd.DataFrame()\n",
    "        dfs_dict = {}\n",
    "    return df, existing, dfs_dict, lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    tech_list = ['AAPL', 'MSFT', 'GOOGL']\n",
    "    indices = ['^GSPC', '^DJI', '^IXIC']\n",
    "    cryptos = ['BTC-USD', 'ETH-USD', 'XRP-USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(info):\n",
    "    static_data = {\n",
    "        'country': info.get('country'),\n",
    "        'industry': info.get('industry'),\n",
    "        'industryKey': info.get('industryKey'),\n",
    "        'sector': info.get('sector'),\n",
    "        'sectorKey': info.get('sectorKey'),\n",
    "        'auditRisk': info.get('auditRisk'),\n",
    "        'boardRisk': info.get('boardRisk'),\n",
    "        'compensationRisk': info.get('compensationRisk'),\n",
    "        'shareHolderRightsRisk': info.get('shareHolderRightsRisk'),\n",
    "        'overallRisk': info.get('overallRisk'),\n",
    "        'dividendRate': info.get('dividendRate'),\n",
    "        'dividendYield': info.get('dividendYield'),\n",
    "        'payoutRatio': info.get('payoutRatio'),\n",
    "        'trailingPE': info.get('trailingPE'),\n",
    "        'forwardPE': info.get('forwardPE'),\n",
    "        'priceToSalesTrailing12Months': info.get('priceToSalesTrailing12Months'),\n",
    "        'priceToBook': info.get('priceToBook'),\n",
    "        'enterpriseToRevenue': info.get('enterpriseToRevenue'),\n",
    "        'enterpriseToEbitda': info.get('enterpriseToEbitda'),\n",
    "        'beta': info.get('beta'),\n",
    "        'bookValue': info.get('bookValue'),\n",
    "        'debtToEquity': info.get('debtToEquity'),\n",
    "        'quickRatio': info.get('quickRatio'),\n",
    "        'currentRatio': info.get('currentRatio'),\n",
    "        'returnOnAssets': info.get('returnOnAssets'),\n",
    "        'returnOnEquity': info.get('returnOnEquity'),\n",
    "        'grossMargins': info.get('grossMargins'),\n",
    "        'ebitdaMargins': info.get('ebitdaMargins'),\n",
    "        'operatingMargins': info.get('operatingMargins'),\n",
    "        'totalRevenue': info.get('totalRevenue'),\n",
    "        'netIncomeToCommon': info.get('netIncomeToCommon'),\n",
    "        'ebitda': info.get('ebitda'),\n",
    "        'totalDebt': info.get('totalDebt'),\n",
    "        'totalCash': info.get('totalCash'),\n",
    "        'totalCashPerShare': info.get('totalCashPerShare'),\n",
    "        'freeCashflow': info.get('freeCashflow'),\n",
    "        'operatingCashflow': info.get('operatingCashflow'),\n",
    "        'earningsGrowth': info.get('earningsGrowth'),\n",
    "        'revenueGrowth': info.get('revenueGrowth'),\n",
    "        'grossProfits': info.get('grossProfits'),\n",
    "        'targetHighPrice': info.get('targetHighPrice'),\n",
    "        'targetLowPrice': info.get('targetLowPrice'),\n",
    "        'targetMeanPrice': info.get('targetMeanPrice'),\n",
    "        'targetMedianPrice': info.get('targetMedianPrice'),\n",
    "        'recommendationMean': info.get('recommendationMean'),\n",
    "        'recommendationKey': info.get('recommendationKey'),\n",
    "        'numberOfAnalystOpinions': info.get('numberOfAnalystOpinions'),\n",
    "        'averageAnalystRating': info.get('averageAnalystRating'),\n",
    "        'trailingEps': info.get('trailingEps'),\n",
    "        'forwardEps': info.get('forwardEps'),\n",
    "        'priceEpsCurrentYear': info.get('priceEpsCurrentYear'),\n",
    "        'earningsQuarterlyGrowth': info.get('earningsQuarterlyGrowth'),\n",
    "        'lastSplitFactor': info.get('lastSplitFactor'),\n",
    "        'lastSplitDate': info.get('lastSplitDate'),\n",
    "        'sharesOutstanding': info.get('sharesOutstanding'),\n",
    "        'floatShares': info.get('floatShares'),\n",
    "        'heldPercentInsiders': info.get('heldPercentInsiders'),\n",
    "        'heldPercentInstitutions': info.get('heldPercentInstitutions')\n",
    "    }\n",
    "    return static_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dict, tickers_to_delete_lst, name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(lst, file, dfs_dict, existing_data):\n",
    "    global data_dict, tickers_to_delete_lst, name_list \n",
    "    data_dict = {}\n",
    "    name_list = []\n",
    "    tickers_to_delete_lst = []\n",
    "    end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    max_retries = 10\n",
    "    initial_retry_delay = 10\n",
    "\n",
    "    # Retrieve data for each symbol\n",
    "    for symbol in lst:\n",
    "        print(symbol)\n",
    "        retries = 0\n",
    "        success = False\n",
    "        while retries < max_retries and not success:\n",
    "            try:\n",
    "                # Determine the start date\n",
    "                if symbol in dfs_dict and not dfs_dict[symbol].empty:\n",
    "                    start_date = dfs_dict[symbol]['Date'].iloc[-1].strftime('%Y-%m-%d')\n",
    "                else:\n",
    "                    start_date = '2010-01-02'\n",
    "                    if testing:\n",
    "                        start_date = '2020-01-02'\n",
    "                \n",
    "                if end_date == start_date:\n",
    "                    success = True\n",
    "                    continue\n",
    "                \n",
    "                # Retrieve data from Yahoo Finance\n",
    "                ticker_yf = yf.Ticker(symbol)\n",
    "                info = ticker_yf.info\n",
    "                name = info.get('longName', symbol)  # Fallback to symbol if no name is available\n",
    "                name_list.append({'symbol': symbol, 'name': name})\n",
    "                \n",
    "                df = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False, prepost=True, actions=True)\n",
    "                df.columns = df.columns.get_level_values(0)\n",
    "                \n",
    "                if not df.empty:\n",
    "                    data_dict[symbol] = df\n",
    "                else:\n",
    "                    print(f\"No data found for {symbol}\")\n",
    "\n",
    "                success = True\n",
    "                clear_output(wait=True)\n",
    "            \n",
    "            except Exception as e:\n",
    "                if \"Too Many Requests\" in str(e) or \"Rate limited\" in str(e):\n",
    "                    # Exponential backoff: Double wait time with each retry\n",
    "                    wait_time = initial_retry_delay * (2 ** retries)\n",
    "                    print(f\"Rate limit error for {symbol}. Attempt {retries + 1}/{max_retries} in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    print(f\"Error retrieving data for {symbol}: {str(e)}\")\n",
    "                    tickers_to_delete_lst.append(symbol)\n",
    "                    break  # Other errors: No retry\n",
    "                \n",
    "        if not success and retries >= max_retries:\n",
    "            print(f\"Maximum retries reached for {symbol}. Skipping...\")\n",
    "            tickers_to_delete_lst.append(symbol)\n",
    "\n",
    "    # Merge data into a DataFrame\n",
    "    df_list = []\n",
    "    for symbol, data in data_dict.items():\n",
    "        data['Symbol'] = symbol\n",
    "        name = next(item['name'] for item in name_list if item['symbol'] == symbol)\n",
    "        data['Name'] = name\n",
    "        df_list.append(data)\n",
    "    \n",
    "    # Combine all new data\n",
    "    if df_list:\n",
    "        new_data = pd.concat(df_list, axis=0)\n",
    "    else:\n",
    "        print(\"No new data available to merge\")\n",
    "        new_data = pd.DataFrame()\n",
    "    \n",
    "    # Combine with existing data\n",
    "    if not new_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data])\n",
    "        combined_data = combined_data.sort_values(by=['Symbol', 'Date'])\n",
    "    else:\n",
    "        combined_data = existing_data\n",
    "    \n",
    "    # Save the data\n",
    "    if not combined_data.empty:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "        combined_data.to_csv(file, index=True)\n",
    "        print(f\"Data saved to {file}\")\n",
    "    \n",
    "    return combined_data, tickers_to_delete_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO: Modify the code to keep querying until a 'too many requests' response is received. Then save the last variable and restart after a time X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_tickers(lst, file, df_symbols):\n",
    "    print(df_symbols)\n",
    "    try:\n",
    "        df_symbols = df_symbols[~df_symbols[file].isin(lst)]\n",
    "\n",
    "        if os.path.exists(f'{file}_new.csv'):\n",
    "            os.remove(f'{file}_new.csv')\n",
    "\n",
    "        df_symbols.to_csv(f'{file}_new.csv', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the CSV file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL', 'MSFT', 'GOOGL']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to cryptos_data_testing.csv\n",
      "Empty DataFrame\n",
      "Columns: [cryptos]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "if testing:\n",
    "    add = '_testing'\n",
    "else:\n",
    "    add = ''\n",
    "\n",
    "file_stocks= f'stock_data{add}.csv'\n",
    "file_indices= f'indices_data{add}.csv'\n",
    "file_cryptos= f'cryptos_data{add}.csv'\n",
    "file_stocks_symbol= 'ticker'\n",
    "file_indices_symbol= 'indices'\n",
    "file_cryptos_symbol= 'cryptos'\n",
    "\n",
    "\n",
    "if not testing:\n",
    "    df_tickers_symbol, existing_data_stocks, dfs_dict_stocks, tech_list =  get_symbols(file_stocks_symbol, file_stocks)\n",
    "    df_indices_symbol, existing_data_indices, dfs_dict_indices, indices =  get_symbols(file_indices_symbol, file_indices)\n",
    "    df_cryptos_symbol, existing_data_cryptos, dfs_dict_cryptos, cryptos =  get_symbols(file_cryptos_symbol, file_cryptos)\n",
    "\n",
    "\n",
    "df_stocks, delt = get_data(tech_list, file_stocks, dfs_dict_stocks, existing_data_stocks)\n",
    "delete_tickers(delt, file_stocks_symbol, df_tickers_symbol)\n",
    "\n",
    "df_indices, delt = get_data(indices, file_indices, dfs_dict_indices, existing_data_indices)\n",
    "delete_tickers(delt, file_indices_symbol, df_indices_symbol)\n",
    "\n",
    "df_cryptos, delt = get_data(cryptos, file_cryptos, dfs_dict_cryptos, existing_data_cryptos)\n",
    "delete_tickers(delt, file_cryptos_symbol, df_cryptos_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    df = pd.read_csv('stock_data_testing.csv', index_col='Date', parse_dates=True)\n",
    "else:\n",
    "    df = pd.read_csv('stock_data.csv', index_col='Date', parse_dates=True)\n",
    "    df = df.drop(columns='Capital Gains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_candle_patterns(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    if len(data) < 2:\n",
    "        # With only one entry, no patterns can be identified\n",
    "        data['Doji'] = False\n",
    "        data['Hammer'] = False\n",
    "        data['Engulfing Bullish'] = False\n",
    "        data['Engulfing Bearish'] = False\n",
    "        return data\n",
    "\n",
    "    # Doji: Open ≈ Close\n",
    "    data['Doji'] = (abs(data['Open'] - data['Close']) <= 0.001 * data['Adj Close'])\n",
    "\n",
    "    # Hammer: Small candle with a long lower shadow\n",
    "    body = abs(data['Close'] - data['Open'])\n",
    "    lower_shadow = data[['Open', 'Close']].min(axis=1) - data['Low']\n",
    "    upper_shadow = data['High'] - data[['Open', 'Close']].max(axis=1)\n",
    "    data['Hammer'] = (lower_shadow > 2 * body) & (upper_shadow < body)\n",
    "\n",
    "    # Engulfing Bullish: Current candle larger and engulfs previous day (bullish)\n",
    "    prev_open = data['Open'].shift(1)\n",
    "    prev_close = data['Close'].shift(1)\n",
    "    data['Engulfing Bullish'] = (\n",
    "        (prev_close < prev_open) &\n",
    "        (data['Close'] > data['Open']) &\n",
    "        (data['Open'] < prev_close) &\n",
    "        (data['Close'] > prev_open)\n",
    "    )\n",
    "\n",
    "    # Engulfing Bearish: Opposite\n",
    "    data['Engulfing Bearish'] = (\n",
    "        (prev_close > prev_open) &\n",
    "        (data['Close'] < data['Open']) &\n",
    "        (data['Open'] > prev_close) &\n",
    "        (data['Close'] < prev_open)\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    # --- Basic Returns & Trends ---\n",
    "    data['Daily Return'] = data['Adj Close'].pct_change()\n",
    "    data['Cumulative Return'] = (1 + data['Daily Return']).cumprod()\n",
    "    data['50 Day MA'] = data['Adj Close'].rolling(window=50).mean()\n",
    "    data['200 Day MA'] = data['Adj Close'].rolling(window=200).mean()\n",
    "    data['20 Day MA'] = data['Adj Close'].rolling(window=20).mean()\n",
    "    data['20 Day STD'] = data['Adj Close'].rolling(window=20).std()\n",
    "\n",
    "    # --- Bollinger Bands ---\n",
    "    data['Upper Band'] = data['20 Day MA'] + (data['20 Day STD'] * 2)\n",
    "    data['Lower Band'] = data['20 Day MA'] - (data['20 Day STD'] * 2)\n",
    "\n",
    "    # --- RSI ---\n",
    "    delta = data['Adj Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # --- MACD ---\n",
    "    ema12 = data['Adj Close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = data['Adj Close'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD'] = ema12 - ema26\n",
    "    data['Signal Line'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # --- Momentum & ROC ---\n",
    "    data['Momentum_10'] = data['Adj Close'] - data['Adj Close'].shift(10)\n",
    "    data['ROC_10'] = data['Adj Close'].pct_change(periods=10)\n",
    "\n",
    "    # --- Stochastic Oscillator ---\n",
    "    low_14 = data['Low'].rolling(window=14).min()\n",
    "    high_14 = data['High'].rolling(window=14).max()\n",
    "    data['%K'] = 100 * ((data['Adj Close'] - low_14) / (high_14 - low_14))\n",
    "    data['%D'] = data['%K'].rolling(window=3).mean()\n",
    "\n",
    "    # --- Williams %R ---\n",
    "    data['Williams_%R'] = -100 * ((high_14 - data['Adj Close']) / (high_14 - low_14))\n",
    "\n",
    "    # --- ATR ---\n",
    "    tr1 = data['High'] - data['Low']\n",
    "    tr2 = abs(data['High'] - data['Adj Close'].shift())\n",
    "    tr3 = abs(data['Low'] - data['Adj Close'].shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    data['ATR_14'] = tr.rolling(window=14).mean()\n",
    "\n",
    "    # --- OBV ---\n",
    "    data['OBV'] = (np.sign(data['Daily Return']) * data['Volume']).fillna(0).cumsum()\n",
    "\n",
    "    # --- Relative Volume ---\n",
    "    data['Relative Volume'] = data['Volume'] / data['Volume'].rolling(20).mean()\n",
    "\n",
    "    # --- Dividends & Splits ---\n",
    "    data['Cumulative Dividends'] = data['Dividends'].cumsum()\n",
    "    data['Has Dividend'] = data['Dividends'] > 0\n",
    "    data['Had Split'] = data['Stock Splits'] > 0\n",
    "\n",
    "    # --- Trend Indicator ---\n",
    "    data['MA_Trend'] = np.where(data['50 Day MA'] > data['200 Day MA'], 1, 0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend_label(data, horizon=5, threshold=0.02):\n",
    "    \"\"\"\n",
    "    horizon: How many days into the future to look?\n",
    "    threshold: Threshold for price change (e.g., 2%)\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    future_return = data['Adj Close'].shift(-horizon) / data['Adj Close'] - 1\n",
    "    data['Trend'] = 0\n",
    "    data.loc[future_return > threshold, 'Trend'] = 1  # Uptrend\n",
    "    data.loc[future_return < -threshold, 'Trend'] = -1  # Downtrend\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_candlestick_with_bollinger(data, symbol=\"\"):\n",
    "    fig = go.Figure(data=[\n",
    "        go.Candlestick(x=data.index,\n",
    "                       open=data['Open'], high=data['High'],\n",
    "                       low=data['Low'], close=data['Close'],\n",
    "                       name='Candles'),\n",
    "        go.Scatter(x=data.index, y=data['Upper Band'], name='Upper Band', line=dict(color='blue', width=1)),\n",
    "        go.Scatter(x=data.index, y=data['Lower Band'], name='Lower Band', line=dict(color='blue', width=1)),\n",
    "        go.Scatter(x=data.index, y=data['20 Day MA'], name='20MA', line=dict(color='orange', width=1)),\n",
    "    ])\n",
    "    fig.update_layout(title=f'{symbol} - Candlestick with Bollinger Bands',\n",
    "                      xaxis_title='Date', yaxis_title='Price')\n",
    "    fig.show()\n",
    "\n",
    "def plot_rsi(data):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(data['RSI'], label='RSI', color='purple')\n",
    "    plt.axhline(70, color='red', linestyle='--')\n",
    "    plt.axhline(30, color='green', linestyle='--')\n",
    "    plt.title('Relative Strength Index (RSI)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_heikin_ashi(data):\n",
    "    ha = data.copy()\n",
    "    ha['HA_Close'] = (data['Open'] + data['High'] + data['Low'] + data['Close']) / 4\n",
    "    ha['HA_Open'] = (data['Open'] + data['Close']) / 2\n",
    "    for i in range(1, len(ha)):\n",
    "        ha.iloc[i, ha.columns.get_loc('HA_Open')] = (ha.iloc[i-1]['HA_Open'] + ha.iloc[i-1]['HA_Close']) / 2\n",
    "    ha['HA_High'] = ha[['High', 'HA_Open', 'HA_Close']].max(axis=1)\n",
    "    ha['HA_Low'] = ha[['Low', 'HA_Open', 'HA_Close']].min(axis=1)\n",
    "    return ha\n",
    "\n",
    "def add_adx(data, period=14):\n",
    "    delta_high = data['High'].diff()\n",
    "    delta_low = data['Low'].diff()\n",
    "    \n",
    "    plus_dm = np.where((delta_high > delta_low) & (delta_high > 0), delta_high, 0)\n",
    "    minus_dm = np.where((delta_low > delta_high) & (delta_low > 0), delta_low, 0)\n",
    "\n",
    "    tr1 = data['High'] - data['Low']\n",
    "    tr2 = abs(data['High'] - data['Close'].shift())\n",
    "    tr3 = abs(data['Low'] - data['Close'].shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "\n",
    "    atr = pd.Series(tr).rolling(window=14).mean()\n",
    "    plus_di = 100 * pd.Series(plus_dm).rolling(window=14).mean() / atr\n",
    "    minus_di = 100 * pd.Series(minus_dm).rolling(window=14).mean() / atr\n",
    "    dx = (abs(plus_di - minus_di) / (plus_di + minus_di)) * 100\n",
    "    adx = dx.rolling(window=14).mean()\n",
    "\n",
    "    data['ADX'] = adx\n",
    "    return data\n",
    "\n",
    "def add_ichimoku(data):\n",
    "    high_9 = data['High'].rolling(window=9).max()\n",
    "    low_9 = data['Low'].rolling(window=9).min()\n",
    "    data['Tenkan-sen'] = (high_9 + low_9) / 2\n",
    "\n",
    "    high_26 = data['High'].rolling(window=26).max()\n",
    "    low_26 = data['Low'].rolling(window=26).min()\n",
    "    data['Kijun-sen'] = (high_26 + low_26) / 2\n",
    "\n",
    "    data['Senkou Span A'] = ((data['Tenkan-sen'] + data['Kijun-sen']) / 2).shift(26)\n",
    "\n",
    "    high_52 = data['High'].rolling(window=52).max()\n",
    "    low_52 = data['Low'].rolling(window=52).min()\n",
    "    data['Senkou Span B'] = ((high_52 + low_52) / 2).shift(26)\n",
    "\n",
    "    data['Chikou Span'] = data['Close'].shift(-26)\n",
    "    return data\n",
    "\n",
    "def add_zigzag(data, window=5):\n",
    "    local_max = argrelextrema(data['Close'].values, np.greater_equal, order=window)[0]\n",
    "    local_min = argrelextrema(data['Close'].values, np.less_equal, order=window)[0]\n",
    "    data['ZigZag'] = np.nan\n",
    "    data.iloc[local_max, data.columns.get_loc('ZigZag')] = data.iloc[local_max, data.columns.get_loc('Close')]\n",
    "    data.iloc[local_min, data.columns.get_loc('ZigZag')] = data.iloc[local_min, data.columns.get_loc('Close')]\n",
    "    # data.iloc[local_max, 'ZigZag'] = data.iloc[local_max, 'Close']\n",
    "    # data.iloc[local_min, 'ZigZag'] = data.iloc[local_min, 'Close']\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_data(data):\n",
    "    data = add_technical_indicators(data)\n",
    "    data = add_candle_patterns(data)\n",
    "    data = add_trend_label(data)\n",
    "    data = add_heikin_ashi(data)\n",
    "    data = add_adx(data, period=14)\n",
    "    data = add_ichimoku(data)\n",
    "    data = add_zigzag(data, window=5)\n",
    "    #plot_candlestick_with_bollinger(data, 'APPL')\n",
    "    data = data.dropna(subset=['200 Day MA'])\n",
    "    data = data.dropna(axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro = df.groupby('Symbol', group_keys=False).apply(preprocess_stock_data, include_groups=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Scaling\n",
    "\n",
    "🎯 Goal: Machine Learning / Feature Engineering\n",
    "✅ 1. Min-Max Scaling (0–1 Normalization)\n",
    "For algorithms like KNN, neural networks, or SVMs:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "```\n",
    "Advantages: Preserves structure, bounded input.\n",
    "Caution: Outliers can significantly distort scaling.\n",
    "\n",
    "✅ 2. Z-Standardization (StandardScaler)\n",
    "For models that rely on normal distribution (e.g., linear models):\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "```\n",
    "Advantages: Centers data at 0 with variance 1 – important for gradient-based methods.\n",
    "\n",
    "✅ 3. Log Scaling\n",
    "Helps to tame extreme differences (e.g., volume or highly volatile stock prices):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "df['Log_Close'] = np.log1p(df['Close'])  # log(1 + x) to allow for 0\n",
    "```\n",
    "Typical for: Volume, market capitalization, price trends over decades.\n",
    "\n",
    "✅ 4. Relative Price Changes (Returns)\n",
    "Instead of absolute prices, percentage changes are considered:\n",
    "\n",
    "```python\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "```\n",
    "Or cumulative:\n",
    "\n",
    "```python\n",
    "df['Cumulative Return'] = (1 + df['Return']).cumprod()\n",
    "```\n",
    "Particularly useful for: Time series models, portfolio comparisons, ML with temporal context.\n",
    "\n",
    "✅ 5. Normalization to First Value\n",
    "To make prices comparable (e.g., comparing multiple stocks synchronously):\n",
    "\n",
    "```python\n",
    "df['Norm_Close'] = df['Close'] / df['Close'].iloc[0]\n",
    "```\n",
    "All prices start at 1. Great for visualization & comparison!\n",
    "\n",
    "🎯 Goal: Deep Learning / RNN / LSTM / TimeSeriesForecasting\n",
    "Perform scaling separately for each stock (symbol)!\n",
    "\n",
    "Fit scaling only on training data, then apply to validation/test data.\n",
    "\n",
    "Caution with leakage! Never use future values when scaling.\n",
    "\n",
    "## Available Signals\n",
    "- Adj Close\n",
    "- Close\n",
    "- Dividends\n",
    "- High\n",
    "- Low\n",
    "- Open\n",
    "- Stock Splits\n",
    "- Volume\n",
    "- Name\n",
    "- Daily Return\n",
    "- Cumulative Return\n",
    "- 50 Day MA\n",
    "- 200 Day MA\n",
    "- 20 Day MA\n",
    "- 20 Day STD\n",
    "- Upper Band\n",
    "- Lower Band\n",
    "- RSI\n",
    "- MACD\n",
    "- Signal Line\n",
    "- Momentum_10\n",
    "- ROC_10\n",
    "- %K\n",
    "- %D\n",
    "- Williams_%R\n",
    "- ATR_14\n",
    "- OBV\n",
    "- Relative Volume\n",
    "- Cumulative Dividends\n",
    "- Has Dividend\n",
    "- Had Split\n",
    "- MA_Trend\n",
    "- Doji\n",
    "- Hammer\n",
    "- Engulfing Bullish\n",
    "- Engulfing Bearish\n",
    "- Trend\n",
    "- HA_Close\n",
    "- HA_Open\n",
    "- HA_High\n",
    "- HA_Low\n",
    "- Tenkan-sen\n",
    "- Kijun-sen\n",
    "- Senkou Span A\n",
    "- Senkou Span B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3402 entries, 2020-10-15 to 2025-04-22\n",
      "Data columns (total 45 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Adj Close             3402 non-null   float64\n",
      " 1   Close                 3402 non-null   float64\n",
      " 2   Dividends             3402 non-null   float64\n",
      " 3   High                  3402 non-null   float64\n",
      " 4   Low                   3402 non-null   float64\n",
      " 5   Open                  3402 non-null   float64\n",
      " 6   Stock Splits          3402 non-null   float64\n",
      " 7   Volume                3402 non-null   int64  \n",
      " 8   Name                  3402 non-null   object \n",
      " 9   Daily Return          3402 non-null   float64\n",
      " 10  Cumulative Return     3402 non-null   float64\n",
      " 11  50 Day MA             3402 non-null   float64\n",
      " 12  200 Day MA            3402 non-null   float64\n",
      " 13  20 Day MA             3402 non-null   float64\n",
      " 14  20 Day STD            3402 non-null   float64\n",
      " 15  Upper Band            3402 non-null   float64\n",
      " 16  Lower Band            3402 non-null   float64\n",
      " 17  RSI                   3402 non-null   float64\n",
      " 18  MACD                  3402 non-null   float64\n",
      " 19  Signal Line           3402 non-null   float64\n",
      " 20  Momentum_10           3402 non-null   float64\n",
      " 21  ROC_10                3402 non-null   float64\n",
      " 22  %K                    3402 non-null   float64\n",
      " 23  %D                    3402 non-null   float64\n",
      " 24  Williams_%R           3402 non-null   float64\n",
      " 25  ATR_14                3402 non-null   float64\n",
      " 26  OBV                   3402 non-null   float64\n",
      " 27  Relative Volume       3402 non-null   float64\n",
      " 28  Cumulative Dividends  3402 non-null   float64\n",
      " 29  Has Dividend          3402 non-null   bool   \n",
      " 30  Had Split             3402 non-null   bool   \n",
      " 31  MA_Trend              3402 non-null   int32  \n",
      " 32  Doji                  3402 non-null   bool   \n",
      " 33  Hammer                3402 non-null   bool   \n",
      " 34  Engulfing Bullish     3402 non-null   bool   \n",
      " 35  Engulfing Bearish     3402 non-null   bool   \n",
      " 36  Trend                 3402 non-null   int64  \n",
      " 37  HA_Close              3402 non-null   float64\n",
      " 38  HA_Open               3402 non-null   float64\n",
      " 39  HA_High               3402 non-null   float64\n",
      " 40  HA_Low                3402 non-null   float64\n",
      " 41  Tenkan-sen            3402 non-null   float64\n",
      " 42  Kijun-sen             3402 non-null   float64\n",
      " 43  Senkou Span A         3402 non-null   float64\n",
      " 44  Senkou Span B         3402 non-null   float64\n",
      "dtypes: bool(6), float64(35), int32(1), int64(2), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_prepro.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Adj Close', 'Close', 'Dividends', 'High', 'Low', 'Open',\n",
       "       'Stock Splits', 'Volume', 'Name', 'Daily Return', 'Cumulative Return',\n",
       "       '50 Day MA', '200 Day MA', '20 Day MA', '20 Day STD', 'Upper Band',\n",
       "       'Lower Band', 'RSI', 'MACD', 'Signal Line', 'Momentum_10', 'ROC_10',\n",
       "       '%K', '%D', 'Williams_%R', 'ATR_14', 'OBV', 'Relative Volume',\n",
       "       'Cumulative Dividends', 'Has Dividend', 'Had Split', 'MA_Trend', 'Doji',\n",
       "       'Hammer', 'Engulfing Bullish', 'Engulfing Bearish', 'Trend', 'HA_Close',\n",
       "       'HA_Open', 'HA_High', 'HA_Low', 'ADX', 'Tenkan-sen', 'Kijun-sen',\n",
       "       'Senkou Span A', 'Senkou Span B', 'Chikou Span', 'ZigZag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue here. Consider whether additional indicators are necessary. Then save the current data and visually select the best indicators.\n",
    "# Create an indicator that considers each sector, adds each stock in equal proportion, and determines the sector's movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Exploratory Data Analysis (EDA)\n",
    "Goals of EDA:\n",
    "- Understand the distribution and patterns in the data.\n",
    "- Identify correlations and potential predictors for stock prices.\n",
    "- Detect outliers or anomalies.\n",
    "Methods:\n",
    "Descriptive Statistics:\n",
    "- Mean, median, standard deviation of stock prices and volume.\n",
    "- Frequency of news or X posts per day.\n",
    "Visualization:\n",
    "- Time series plots of stock prices.\n",
    "- Correlation matrix between stock prices, macroeconomic data, and sentiment.\n",
    "- Boxplots to identify outliers (e.g., extreme price movements).\n",
    "Correlations:\n",
    "- Check whether stock prices correlate with macroeconomic indicators (e.g., interest rates) or sentiment data.\n",
    "- Analyze the relationship between trading volume and price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notes**\n",
    "####\n",
    "- Create multiple dashboards:\n",
    "    1. Dashboard: Price movement prediction\n",
    "    2. Dashboard: Identify potential trends based on fundamental analysis, news, etc., and identify stocks as promising based on these data --> do not consider price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
